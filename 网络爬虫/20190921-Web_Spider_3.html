<!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="https://fonts.loli.net/css?family=Noto+Serif+SC:400,500,700&display=swap&subset=chinese-simplified" rel="stylesheet"><link href="https://fonts.loli.net/css?family=Parisienne" rel="stylesheet"><link href="https://fonts.loli.net/css?family=Pinyon+Script" rel="stylesheet"><link href="https://fonts.loli.net/css?family=EB+Garamond:400,500,700" rel="stylesheet"><link href="https://fonts.loli.net/css?family=Source+Code+Pro:400,700" rel="stylesheet"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/logo-32x32.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/logo-16x16.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="学习笔记,python,网络爬虫,"><meta name="description" content="本篇介绍基于requests的爬虫。"><meta name="keywords" content="学习笔记,python,网络爬虫"><meta property="og:type" content="article"><meta property="og:title" content="基于requests的爬虫"><meta property="og:url" content="https://chennq.top/网络爬虫/20190921-Web_Spider_3.html"><meta property="og:site_name" content="Naqin"><meta property="og:description" content="本篇介绍基于requests的爬虫。"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2019-11-04T17:20:15.426Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="基于requests的爬虫"><meta name="twitter:description" content="本篇介绍基于requests的爬虫。"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.4",sidebar:{position:"left",display:"hide",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1,dimmer:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!0,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://chennq.top/网络爬虫/20190921-Web_Spider_3.html"><title>基于requests的爬虫 | Naqin</title><meta name="baidu-site-verification" content="rqAw8UHNKS"><meta name="google-site-verification" content="r_iyjm0f8KcvC9kIpQiyZx3pmWlLY-gwBv9KsOF2AYo"></head><body itemscope itemtype="https://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="https://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Naqin</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Run! Forest,run!</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section">关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger">搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="https://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://chennq.top/网络爬虫/20190921-Web_Spider_3.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Naqin"><meta itemprop="description" content><meta itemprop="image" content="/uploads/tu6.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Naqin"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">基于requests的爬虫</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-21T10:19:08+08:00">2019-09-21 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/网络爬虫/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a></span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">6.6k 字</span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><p>本篇介绍基于requests的爬虫。</p></blockquote><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>概念：基于网络请求的模块</p><p>作用：用来模拟浏览器发请求，从而实现爬虫</p><h1 id="通用爬虫"><a href="#通用爬虫" class="headerlink" title="通用爬虫"></a>通用爬虫</h1><p>步骤：</p><ol><li>指定url</li><li>请求发送：get返回的是一个响应对象</li><li>获取响应数据: text返回的是字符串形式的响应数据</li><li>持久化存储</li></ol><h2 id="爬取搜狗首页的页面源码数据"><a href="#爬取搜狗首页的页面源码数据" class="headerlink" title="爬取搜狗首页的页面源码数据"></a>爬取搜狗首页的页面源码数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 1.指定url</span></span><br><span class="line">url = <span class="string">'https://www.sogou.com/'</span></span><br><span class="line"><span class="comment"># 2.请求发送：get返回的是一个响应对象</span></span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"><span class="comment"># 3. 获取响应数据: text返回的是字符串形式的响应数据</span></span><br><span class="line">page_text = response.text</span><br><span class="line">page_text</span><br><span class="line"><span class="comment"># 4. 持久化存储</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./sogou.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure><h2 id="实现一个简易的网页采集器"><a href="#实现一个简易的网页采集器" class="headerlink" title="实现一个简易的网页采集器"></a>实现一个简易的网页采集器</h2><ul><li>请求参数动态化（自定义字典给get方法的params传参）</li><li>使用UA伪装</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.sogou.com/web'</span></span><br><span class="line"><span class="comment"># 请求参数的动态化</span></span><br><span class="line">wd = input(<span class="string">'enter a key word：'</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'query'</span>:wd</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 没有做UA伪装是得不到数据的</span></span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url, params=params,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将响应数据的编码格式手动进行指定，来解决乱码问题。</span></span><br><span class="line">response.encoding = <span class="string">'utf-8'</span></span><br><span class="line">page_text = response.text</span><br><span class="line">fileName = wd + <span class="string">'.html'</span></span><br><span class="line"><span class="keyword">with</span> open(fileName,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(page_text)</span><br><span class="line">print(fileName,<span class="string">'爬取成功！'</span>)</span><br></pre></td></tr></table></figure><h2 id="动态加载的数据"><a href="#动态加载的数据" class="headerlink" title="动态加载的数据"></a>动态加载的数据</h2><ul><li>页面中想要爬取的内容并不是请求当前url得到的，而是通过另一个网络请求请求到的数据（例如，滚轮滑到底部，会发送ajax，对局部进行刷新）</li></ul><p>例子：爬取豆瓣电影中动态加载出的电影详情</p><p>我们想爬取的页面是：豆瓣电影分类排行榜 - 科幻片 <code>https://movie.douban.com/typerank?type_name=%E7%A7%91%E5%B9%BB&amp;type=17&amp;interval_id=100:90&amp;action=</code></p><ul><li><p>首先在chorme的抓包工具中进行全局搜索发现它不是请求当前url得到的，而是一个新的url： <code>https://movie.douban.com/j/chart/top_list?</code></p></li><li><p>当然这是一个ajax请求（在chorme的抓包工具中选择 XHR 可以进行查看）</p></li><li><p>在寻找另一部电影，请求的url仍然是 <code>https://movie.douban.com/j/chart/top_list?</code></p></li><li><p>然后我们就需要找参数的规律了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">type: <span class="number">17</span></span><br><span class="line">interval_id: <span class="number">100</span>:<span class="number">90</span></span><br><span class="line">action: </span><br><span class="line">start: <span class="number">0</span></span><br><span class="line">limit: <span class="number">1</span></span><br><span class="line"><span class="comment"># 第二部</span></span><br><span class="line">type: <span class="number">17</span></span><br><span class="line">interval_id: <span class="number">100</span>:<span class="number">90</span></span><br><span class="line">action: </span><br><span class="line">start: <span class="number">20</span></span><br><span class="line">limit: <span class="number">20</span></span><br></pre></td></tr></table></figure></li><li><p>start 和 limit 是可以变化的，基于例二，我们自定义字典进行传参，然后进行尝试。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://movie.douban.com/j/chart/top_list'</span></span><br><span class="line"><span class="comment"># 参数动态化</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'type'</span>: <span class="string">'5'</span>,</span><br><span class="line">    <span class="string">'interval_id'</span>: <span class="string">'100:90'</span>,</span><br><span class="line">    <span class="string">'action'</span>: <span class="string">''</span>,</span><br><span class="line">    <span class="string">'start'</span>: <span class="string">'01'</span>,</span><br><span class="line">    <span class="string">'limit'</span>: <span class="string">'20'</span>,	</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># start 决定起始位置，limit 为显示数量。</span></span><br><span class="line">response = requests.get(url=url, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># json() 返回序列化好的对象（字典，列表）</span></span><br><span class="line">movie_list = response.json()  </span><br><span class="line"><span class="comment"># 手动反序列化</span></span><br><span class="line"><span class="comment"># import json</span></span><br><span class="line"><span class="comment"># movie_list = json.loads(movie_list)</span></span><br><span class="line"><span class="keyword">for</span> movie <span class="keyword">in</span> movie_list:</span><br><span class="line">    print(movie[<span class="string">'title'</span>],movie[<span class="string">'score'</span>])</span><br></pre></td></tr></table></figure><p>note：response.json() 可以免除我们手动使用json进行loads的过程。</p><h2 id="爬取肯德基的餐厅位置信息"><a href="#爬取肯德基的餐厅位置信息" class="headerlink" title="爬取肯德基的餐厅位置信息"></a>爬取肯德基的餐厅位置信息</h2><ul><li><p>地址为：<code>http://www.kfc.com.cn/kfccda/storelist/index.aspx</code></p></li><li><p>post请求</p></li><li><p>一次爬取多页数据</p></li></ul><p>思路：</p><ul><li>首先，数据是动态加载的，分析请求方式为 post 发出的 url 与 data。</li><li>通过pageSize就可以在循环内完成多所有地址的爬取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'cname'</span>:<span class="string">''</span>, </span><br><span class="line">    <span class="string">'keyword'</span>:<span class="string">'北京'</span>,</span><br><span class="line">    <span class="string">'pageIndex'</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">'pageSize'</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">'pid'</span>:<span class="string">''</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(data[<span class="string">'keyword'</span>]+<span class="string">'肯德基地址.txt'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,data[<span class="string">'pageSize'</span>]+<span class="number">1</span>):</span><br><span class="line">        data[<span class="string">'pageIndex'</span>] = i    </span><br><span class="line">        address_dic = requests.post(url=url, data=data, headers=headers).json()</span><br><span class="line">        <span class="keyword">for</span> address <span class="keyword">in</span> address_dic[<span class="string">'Table1'</span>]:</span><br><span class="line">            print(address[<span class="string">'addressDetail'</span>])</span><br><span class="line">            f.write(address[<span class="string">'addressDetail'</span>] + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p>补充：可以使用代理服务器，如搜索：全网代理IP</p><h2 id="中标公告提取"><a href="#中标公告提取" class="headerlink" title="中标公告提取"></a>中标公告提取</h2><ul><li>需求<br><a href="https://www.fjggfw.gov.cn/Website/JYXXNew.aspx" target="_blank" rel="noopener">https://www.fjggfw.gov.cn/Website/JYXXNew.aspx</a> 福建省公共资源交易中心</li></ul><p>提取内容:</p><p>工程建设中的中标结果信息/中标候选人信息</p><ol><li><p>完整的html中标信息</p></li><li><p>第一中标候选人</p></li><li><p>中标金额</p></li><li><p>中标时间</p></li><li><p>其它参与投标的公司</p></li></ol><p>思路：</p><ol><li>从首页打开一个公告，先尝试得到一个公告的信息。</li><li>在得到另一个公告的信息进行比较，只有ID是变化的，所以有了ID就可以批量爬取了。</li><li>回到首页，判断加载方式，确定数据的请求方式，url以及参数。</li></ol><p>实现过程：</p><ul><li>确认爬取的数据都是动态加载出来的</li><li>在首页中捕获到ajax请求对应的数据包，从该数据包中提取出请求的url和请求参数</li><li>对提取到的url进行请求发送，获取响应数据（json），（包含ID信息）</li><li>从json串中提取到每一个公告对应的id值</li><li>将id值和中标信息对应的url进行整合，进行请求发送捕获到每一个公告对应的中标信息数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该部分为得到一个公告的信息，但是我们这里的ID不灵活，需要进一步改进。</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'OPtype'</span>: <span class="string">'GetGGInfoPC'</span>,</span><br><span class="line">    <span class="string">'ID'</span>: <span class="number">132458</span>,</span><br><span class="line">    <span class="string">'GGTYPE'</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">'url'</span>: <span class="string">'AjaxHandler/BuilderHandler.ashx'</span>,</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'ASP.NET_SessionId=j0ps3kcjisaajyenlgihwwxo; Hm_lvt_94bfa5b89a33cebfead2f88d38657023=1570523570; Hm_lpvt_94bfa5b89a33cebfead2f88d38657023=1570523621; _qddagsx_02095bad0b=115b25431c8f1a2f7f607e8464ba7c5ef5807a77e65a44aa3c9045306ab0ba3bf02c48523e97b816f16d9ff0c57b6e77f46e59f8776b88c64cbd9da7f84676d8c4c9db3686235ef49e9ee7ff1871ec99884e7ba79b8c173e472b039b0c9a8fb61b4049ab036f68e1f5e0857c4bd4131f0c3b7478b98687d0b9c0538352871ec9'</span>,</span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://www.fjggfw.gov.cn/Website/AjaxHandler/BuilderHandler.ashx'</span></span><br><span class="line">response = requests.get(url=url, params=params, headers=headers)</span><br><span class="line">response.text</span><br></pre></td></tr></table></figure><p>完成了对公告详情的爬取后，接下来批量爬取公告。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该部分完成了对前5页的公告信息进行爬取</span></span><br><span class="line">url = <span class="string">'https://www.fjggfw.gov.cn/Website/AjaxHandler/BuilderHandler.ashx'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'ASP.NET_SessionId=j0ps3kcjisaajyenlgihwwxo; Hm_lvt_94bfa5b89a33cebfead2f88d38657023=1570523570; Hm_lpvt_94bfa5b89a33cebfead2f88d38657023=1570523621; _qddagsx_02095bad0b=115b25431c8f1a2f7f607e8464ba7c5ef5807a77e65a44aa3c9045306ab0ba3bf02c48523e97b816f16d9ff0c57b6e77f46e59f8776b88c64cbd9da7f84676d8c4c9db3686235ef49e9ee7ff1871ec99884e7ba79b8c173e472b039b0c9a8fb61b4049ab036f68e1f5e0857c4bd4131f0c3b7478b98687d0b9c0538352871ec9'</span>,</span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'OPtype'</span>: <span class="string">'GetListNew'</span>,</span><br><span class="line">    <span class="string">'pageNo'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'pageSize'</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">'proArea'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'category'</span>: <span class="string">'GCJS'</span>,</span><br><span class="line">    <span class="string">'announcementType'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'ProType'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'xmlx'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'projectName'</span>: <span class="string">''</span>,</span><br><span class="line">    <span class="string">'TopTime'</span>: <span class="string">'2019-07-10 00:00:00'</span>,</span><br><span class="line">    <span class="string">'EndTime'</span>: <span class="string">'2019-10-08 23:59:59'</span>,</span><br><span class="line">    <span class="string">'rrr'</span>: <span class="number">0.8853761868569314</span>,</span><br><span class="line">&#125;</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'OPtype'</span>: <span class="string">'GetGGInfoPC'</span>,</span><br><span class="line">    <span class="string">'ID'</span>: <span class="number">132458</span>,</span><br><span class="line">    <span class="string">'GGTYPE'</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">'url'</span>: <span class="string">'AjaxHandler/BuilderHandler.ashx'</span>,</span><br><span class="line">&#125;</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    data[<span class="string">'pageNo'</span>] = i    </span><br><span class="line">    <span class="comment"># 第i页内容的爬取</span></span><br><span class="line">    response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line">    post_data = response.json()</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> post_data[<span class="string">'data'</span>]:</span><br><span class="line">        ID = int(row[<span class="string">'M_ID'</span>])</span><br><span class="line">        params[<span class="string">'ID'</span>] = ID</span><br><span class="line">        company_respose = requests.get(url=url, params=params, headers=headers)</span><br><span class="line">        company_detail = company_respose.json()[<span class="string">'data'</span>]</span><br><span class="line"><span class="comment">#         print(company_detail)</span></span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">print(count)</span><br></pre></td></tr></table></figure><h2 id="爬取图片"><a href="#爬取图片" class="headerlink" title="爬取图片"></a>爬取图片</h2><p>如何做？</p><ul><li>基于requests</li><li>基于urllib</li><li>区别：urllib中的 urlretrieve 不可以进行UA伪装<br>requests在urllib基础上产生，更加pythonic！</li></ul><h3 id="基于requests的图片爬取"><a href="#基于requests的图片爬取" class="headerlink" title="基于requests的图片爬取"></a>基于requests的图片爬取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 基于requests的图片爬取</span></span><br><span class="line">url = <span class="string">r'https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1570593174006&amp;di=5ade9e1cb9e63a708c69095283f8e40a&amp;imgtype=0&amp;src=http%3A%2F%2Fdingyue.nosdn.127.net%2F92Ot1vmaeOklEbu2G7ABakMeGiYWpYi8R3urPnggBDJSs1535663928397.jpeg'</span></span><br><span class="line">img_data = requests.get(url=url, headers=headers).content   <span class="comment"># content 返回的是bytes类型的响应数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./123.jpg'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(img_data)</span><br></pre></td></tr></table></figure><ul><li>request更具通用性，数据可以展示为：text（字符串），json（列表/字典），content（字节）。</li></ul><h3 id="基于urllib的图片爬取"><a href="#基于urllib的图片爬取" class="headerlink" title="基于urllib的图片爬取"></a>基于urllib的图片爬取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于urllib</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">url = <span class="string">'https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1570593174006&amp;di=5ade9e1cb9e63a708c69095283f8e40a&amp;imgtype=0&amp;src=http%3A%2F%2Fdingyue.nosdn.127.net%2F92Ot1vmaeOklEbu2G7ABakMeGiYWpYi8R3urPnggBDJSs1535663928397.jpeg'</span></span><br><span class="line">request.urlretrieve(url,<span class="string">'./456.jpg'</span>)</span><br></pre></td></tr></table></figure><ul><li>由于urlretrieve不能做UA伪装，所以存在图片缺失的可能。</li></ul><h1 id="聚焦爬虫"><a href="#聚焦爬虫" class="headerlink" title="聚焦爬虫"></a>聚焦爬虫</h1><p>较通用爬虫相比，增加了<strong>数据解析</strong>。</p><h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><p>概念：将一整张页面的局部数据进行提取/解析</p><p>作用：用来实现聚焦爬虫</p><p>实现方式：</p><ul><li>正则：可以匹配任意，下面方式拿不到 js 代码中的数据</li><li>bs4</li><li>xpath</li><li>pyquery</li></ul><p>数据解析的通用原理是什么？</p><ul><li>标签的定位</li><li>数据的提取</li></ul><p>页面中的相关的字符串的数据都存储在哪里？</p><ul><li>标签中间</li><li>标签的属性中</li></ul><p>基于聚焦爬虫的编码流程</p><ul><li>指定url</li><li>发起请求</li><li>获取响应数据</li><li><strong>数据解析</strong></li><li>持久化存储</li></ul><h2 id="正则解析"><a href="#正则解析" class="headerlink" title="正则解析"></a>正则解析</h2><h3 id="爬取煎蛋网中的图片"><a href="#爬取煎蛋网中的图片" class="headerlink" title="爬取煎蛋网中的图片"></a>爬取煎蛋网中的图片</h3><ul><li>地址为：<code>http://jandan.net/pic/MjAxOTEwMDktNjk=#comments</code></li></ul><p>实现过程：</p><ul><li>指定url</li><li>获取响应数据</li><li>数据解析<ul><li>写正则表达式</li><li>正则匹配</li></ul></li><li>持久化存储</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">url = <span class="string">'http://jandan.net/pic/MjAxOTEwMDktNjk=#comments'</span></span><br><span class="line">page_text = requests.get(url=url,headers=headers).text</span><br><span class="line"><span class="comment"># 解析数据：img标签的src的属性值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写正则表达式</span></span><br><span class="line"><span class="comment"># print(page_text) # 参考page_text来定正则，网页源码可能有些小问题</span></span><br><span class="line"><span class="comment"># ex = r'&lt;div class="text"&gt;.*?&lt;img src="(.*?)" referrerpolicy'   # 坑！！！</span></span><br><span class="line">ex = <span class="string">r'&lt;div class="text"&gt;.*?&lt;img src="(.*?)" referrerPolicy'</span>   <span class="comment"># 在源码中那里的p是小写，可爬下来的p是大写，坑~~~</span></span><br><span class="line"></span><br><span class="line">dirName = <span class="string">'./JDimg/'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dirName):</span><br><span class="line">    os.mkdir(dirName)</span><br><span class="line">img_url_list = re.findall(ex,page_text,re.S)   <span class="comment"># re.S处理回车</span></span><br><span class="line"><span class="keyword">for</span> img_url <span class="keyword">in</span> img_url_list:</span><br><span class="line">    ex2 = <span class="string">r'org_src="(.*?)"'</span></span><br><span class="line">    gif_url = re.findall(ex2,img_url)</span><br><span class="line">    <span class="keyword">if</span> gif_url:</span><br><span class="line">        new_url = <span class="string">'http:'</span> + gif_url[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        new_url = <span class="string">'http:'</span> + img_url</span><br><span class="line">    name = new_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">    img = requests.get(new_url,headers=headers).content</span><br><span class="line">    img_path = dirName + name </span><br><span class="line">    <span class="keyword">with</span> open(img_path,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(img)</span><br><span class="line">    print(name,<span class="string">'下载完成!!!'</span>)</span><br></pre></td></tr></table></figure><p>note：内容依据响应数据page_text，而不是根据浏览器中网页上的代码，上面就出现一个字母的大小写不同而导致正则失效的例子。</p><h2 id="bs4解析"><a href="#bs4解析" class="headerlink" title="bs4解析"></a>bs4解析</h2><p>环境的安装：</p><ul><li>pip install bs4</li><li>pip install lxml</li></ul><p>bs4的解析原理：</p><ul><li>实例化一个BeatifulSoup的一个对象，把即将被解析的页面源码数据加载到该对象中</li><li>需要调用BeatifulSoup对象中的相关的<span style="color:red">方法</span>和<span style="color:red">属性</span>进行标签定位和数据的提取</li></ul><p>BeatifulSoup的实例化</p><ul><li>BeatifulSoup(fp, ‘lxml’) 将本地存储的html文档中的页面源码数据加载到该对象中</li><li>BeatifulSoup(page_text, ‘lxml’) 将从互联网中请求到的页面源码数据加载到该对象中</li></ul><p>标签的定位</p><ul><li><p>soup.tagName: 只可以定位到第一个tagName标签</p></li><li><p>属性定位：</p><ul><li>soup.find(‘div’,’attrName=’value’) 只能定位到符合要求的第一个</li><li>soup.findAll：返回列表，可以定位到符合要求的所有标签<br>note：只有class需要加下划线，其它直接用原名就可以。</li></ul></li><li><p>选择器定位：</p><ul><li>select(‘选择器’)</li><li>选择器：id，class，tag，层级选择器（大于号表示一个层级，空格表示多个层级）</li></ul></li><li><p>取文本</p><ul><li>text：将标签中所有文本取出</li><li>string：将标签中直系的文本取出</li></ul></li><li><p>取属性</p><ul><li>tag[‘attrName’]</li></ul></li></ul><h3 id="熟悉"><a href="#熟悉" class="headerlink" title="熟悉"></a>熟悉</h3><p>此为练手的html。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">title</span>&gt;</span>测试bs4<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">p</span>&gt;</span>百里守约<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"song"</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">p</span>&gt;</span>李清照<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">p</span>&gt;</span>王安石<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">p</span>&gt;</span>苏轼<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">p</span>&gt;</span>柳宗元<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.song.com/"</span> <span class="attr">title</span>=<span class="string">"赵匡胤"</span> <span class="attr">target</span>=<span class="string">"_self"</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">span</span>&gt;</span>this is span<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">		宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"du"</span>&gt;</span>总为浮云能蔽日,长安不见使人愁<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://www.baidu.com/meinv.jpg"</span> <span class="attr">alt</span>=<span class="string">""</span> /&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"tang"</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.baidu.com"</span> <span class="attr">title</span>=<span class="string">"qing"</span>&gt;</span>清明时节雨纷纷,路上行人欲断魂,借问酒家何处有,牧童遥指杏花村<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.163.com"</span> <span class="attr">title</span>=<span class="string">"qin"</span>&gt;</span>秦时明月汉时关,万里长征人未还,但使龙城飞将在,不教胡马度阴山<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.126.com"</span> <span class="attr">alt</span>=<span class="string">"qi"</span>&gt;</span>岐王宅里寻常见,崔九堂前几度闻,正是江南好风景,落花时节又逢君<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.sina.com"</span> <span class="attr">class</span>=<span class="string">"du"</span>&gt;</span>杜甫<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.dudu.com"</span> <span class="attr">class</span>=<span class="string">"du"</span>&gt;</span>杜牧<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">b</span>&gt;</span>杜小月<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">i</span>&gt;</span>度蜜月<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.haha.com"</span> <span class="attr">id</span>=<span class="string">"feng"</span>&gt;</span>凤凰台上凤凰游,凤去台空江自流,吴宫花草埋幽径,晋代衣冠成古丘<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>熟悉 BeautifulSoup 的选择器以及如何取文本和属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">fp = open(<span class="string">'./test.html'</span>,encoding=<span class="string">'utf-8'</span>) </span><br><span class="line">soup = BeautifulSoup(fp ,<span class="string">'lxml'</span>)</span><br><span class="line">soup.div				</span><br><span class="line">type(soup.div)                   <span class="comment"># bs4.element.Tag</span></span><br><span class="line">soup.find(<span class="string">'div'</span>)</span><br><span class="line">soup.find(<span class="string">'div'</span>,class_=<span class="string">"song"</span>)   <span class="comment"># 由于class 是关键字，所以需要加下划线</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面的 findAll 与 select 都将返回列表</span></span><br><span class="line">soup.findAll(<span class="string">'div'</span>)              </span><br><span class="line">soup.select(<span class="string">'div'</span>)     </span><br><span class="line">soup.select(<span class="string">'div[class="song"]'</span>) </span><br><span class="line">soup.select(<span class="string">'.song'</span>)</span><br><span class="line">soup.select(<span class="string">'div &gt; ul &gt; li'</span>)</span><br><span class="line">soup.select(<span class="string">'div li'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取文本</span></span><br><span class="line">soup.select(<span class="string">'div li'</span>)[<span class="number">0</span>].text    </span><br><span class="line">soup.select(<span class="string">'.song'</span>)[<span class="number">0</span>].text</span><br><span class="line">soup.select(<span class="string">'.song'</span>)[<span class="number">0</span>].string    <span class="comment"># 只能取直系,所以这个结果为空</span></span><br><span class="line">soup.select(<span class="string">'b'</span>)[<span class="number">0</span>].string      </span><br><span class="line"></span><br><span class="line"><span class="comment"># 取属性</span></span><br><span class="line">soup.select(<span class="string">'div a'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</span><br></pre></td></tr></table></figure><h3 id="爬取三国演义小说的标题和内容"><a href="#爬取三国演义小说的标题和内容" class="headerlink" title="爬取三国演义小说的标题和内容"></a>爬取三国演义小说的标题和内容</h3><ul><li>地址为：<code>http://www.shicimingju.com/book/sanguoyanyi.html</code></li></ul><p>数据解析流程：</p><ul><li>首先定位标签</li><li>然后取出文本和内容</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">main_url = <span class="string">'http://www.shicimingju.com/book/sanguoyanyi.html'</span></span><br><span class="line">page_text = requests.get(url=main_url,headers=headers).text</span><br><span class="line"><span class="comment"># 数据解析: 章节的标题和详情页的url</span></span><br><span class="line">soup = BeautifulSoup(page_text,<span class="string">'lxml'</span>)</span><br><span class="line">li = soup.select(<span class="string">'.book-mulu a'</span>)</span><br><span class="line">fp = open(<span class="string">'./sanguo.txt'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    title = i.string</span><br><span class="line">    detail_url = <span class="string">'http://www.shicimingju.com'</span> + i[<span class="string">'href'</span>]</span><br><span class="line">    detail_page_text = requests.get(url=detail_url,headers=headers).text</span><br><span class="line">    <span class="comment"># 数据解析：章节内容</span></span><br><span class="line">    detail_soup = BeautifulSoup(detail_page_text,<span class="string">'lxml'</span>)</span><br><span class="line">    detail = detail_soup.find(<span class="string">'div'</span>,class_=<span class="string">'chapter_content'</span>).text</span><br><span class="line">    fp.write(title + <span class="string">'\n'</span> + detail + <span class="string">'\n\n'</span>)</span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure><h2 id="xpath-解析"><a href="#xpath-解析" class="headerlink" title="xpath 解析"></a>xpath 解析</h2><ul><li><p>环境的安装</p><ul><li>pip install lxml</li></ul></li><li><p>解析原理</p><ul><li>实例化一个etree的对象，且把即将被解析的页面源码数据加载到该对象中</li><li>调用etree对象中的xpath方法结合着<span class="mark">不同形式的xpath表达式</span>进行标签定位和数据提取</li></ul></li><li><p>etree对象的实例化</p><ul><li>etree.parse(‘fileName’)</li><li>etree.HTML(page_text)</li></ul></li><li><p>标签定位</p><ul><li>最左侧的/：一定要从根标签开始进行标签定位</li><li>非最左侧的/：表示一个层级</li><li>最左侧的//：可以从任意位置进行指定标签的定位</li><li>非最左侧的//：表示多个层级</li><li>属性定位：<ul><li>//tagName[@attrName=”value”]</li></ul></li><li>索引定位：<ul><li>//tagName[@attrName=”value”]/li[2]，索引是从1开始的</li></ul></li><li>逻辑运算：<pre><code>- 找到href属性值为空且class属性值为du的a标签</code></pre><ul><li>//a[@href=”” and @class=”du”]</li></ul></li><li>模糊匹配：<ul><li>//div[contains(@class, “ng”)]</li><li>//div[starts-with(@class, “ta”)]</li></ul></li></ul></li><li><p>取文本</p><ul><li>/text():取直系的文本内容</li><li>//text()：取所有文本内容</li></ul></li><li><p>取属性</p><ul><li>/@attrName</li></ul></li></ul><p>note：</p><ul><li>返回的都是列表</li><li>在google中的Element可以直接copy xpath</li><li>在xpath表达式中不可以出现tbody标签</li></ul><h3 id="熟悉-1"><a href="#熟悉-1" class="headerlink" title="熟悉"></a>熟悉</h3><p>依旧使用上面的html练手</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">tree = etree.parse(<span class="string">'./test.html'</span>)</span><br><span class="line">tree        <span class="comment"># &lt;lxml.etree._ElementTree at 0x231b25371c8&gt;</span></span><br><span class="line">tree.xpath(<span class="string">'/html'</span>)</span><br><span class="line">tree.xpath(<span class="string">'/html//title'</span>)</span><br><span class="line">tree.xpath(<span class="string">'//div'</span>)</span><br><span class="line"><span class="comment"># 属性定位</span></span><br><span class="line">tree.xpath(<span class="string">'//*[@class="tang"]'</span>)</span><br><span class="line">tree.xpath(<span class="string">'//div[@class="song"]'</span>)</span><br><span class="line">tree.xpath(<span class="string">'//div[@class="tang"]/ul/li'</span>)</span><br><span class="line"><span class="comment"># 索引定位</span></span><br><span class="line">tree.xpath(<span class="string">'//div[@class="tang"]/ul/li[2]'</span>)   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 取文本</span></span><br><span class="line">tree.xpath(<span class="string">'//b/text()'</span>)   <span class="comment"># 取直系</span></span><br><span class="line">tree.xpath(<span class="string">'//div[@class="tang"]/ul/li//text()'</span>)   <span class="comment"># 取所有</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取属性</span></span><br><span class="line">tree.xpath(<span class="string">'//a/@href'</span>)</span><br></pre></td></tr></table></figure><h3 id="爬取虎牙主播名称，热度和标题"><a href="#爬取虎牙主播名称，热度和标题" class="headerlink" title="爬取虎牙主播名称，热度和标题"></a>爬取虎牙主播名称，热度和标题</h3><ul><li>地址为：<a href="https://www.huya.com/g/xingxiu" target="_blank" rel="noopener">https://www.huya.com/g/xingxiu</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.huya.com/g/xingxiu'</span></span><br><span class="line">page_text = requests.get(url,headers=headers).text</span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">li_list = tree.xpath(<span class="string">'//div[@class="box-bd"]/ul/li'</span>)</span><br><span class="line"><span class="comment"># print(page_text)</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    title = li.xpath(<span class="string">'./a[2]/@title'</span>)[<span class="number">0</span>] </span><br><span class="line">    name = li.xpath(<span class="string">'./span/span[1]/i/@title'</span>)[<span class="number">0</span>] </span><br><span class="line">    hot = li.xpath(<span class="string">'./span/span[2]/i[2]/text()'</span>)[<span class="number">0</span>] </span><br><span class="line">    print(name,<span class="string">'的直播间: '</span>,title,<span class="string">'热度为:'</span>,hot)</span><br></pre></td></tr></table></figure><h3 id="爬取前5页的妹子"><a href="#爬取前5页的妹子" class="headerlink" title="爬取前5页的妹子"></a>爬取前5页的妹子</h3><p>地址为：<code>http://sc.chinaz.com/tag_tupian/yazhoumeinv.html</code></p><ul><li>涉及中文乱码<ul><li>以前对response对象设置 encoding 这样针对一整张页面代价大。</li><li>我们只需要对想要的内容进行转码。</li><li>通常由 iso-8859-1 –&gt; utf-8 先编码成iso-8859-1 在解码成 utf-8 或gbk</li></ul></li><li>多页码数据的爬取<ul><li>制定一个通用的url模板</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用模板</span></span><br><span class="line">url = <span class="string">'http://sc.chinaz.com/tag_tupian/yazhoumeinv_%s.html'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    <span class="comment"># 获取页面</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">        new_url = <span class="string">'http://sc.chinaz.com/tag_tupian/yazhoumeinv.html'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        new_url = format(url%i)</span><br><span class="line"></span><br><span class="line">    page_text = requests.get(new_url,headers=headers).text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    img_list = tree.xpath(<span class="string">'//*[@id="container"]/div/div/a'</span>)  </span><br><span class="line"></span><br><span class="line">    dirName = (<span class="string">'./MZimg'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dirName):</span><br><span class="line">        os.makedirs(dirName)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./MZ.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(page_text)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> img_list:        </span><br><span class="line">        title = li.xpath(<span class="string">'./img/@alt'</span>)[<span class="number">0</span>].encode(<span class="string">' iso-8859-1'</span>).decode(<span class="string">'utf-8'</span>) + <span class="string">'.jpg'</span>		</span><br><span class="line">        <span class="comment"># print(li.xpath('./img/@src2'))     </span></span><br><span class="line">        <span class="comment"># 由于是惰性加载，所以这里没有值，这该怎么办呢？ 发现这里的惰性加载只是用src2替换src</span></span><br><span class="line">        <span class="comment"># 疑惑: 为什么./@src2 得不到，而必须用 ./img/@src2 呢？  ~~~</span></span><br><span class="line">        </span><br><span class="line">        src = li.xpath(<span class="string">'./img/@src2'</span>)[<span class="number">0</span>]</span><br><span class="line">        img = requests.get(src,headers=headers).content</span><br><span class="line">        <span class="comment"># 数据持久化存储</span></span><br><span class="line">        img_path = dirName + <span class="string">'/'</span> + title    </span><br><span class="line">        <span class="keyword">with</span> open(img_path,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(img)</span><br><span class="line"> </span><br><span class="line">    print(<span class="string">f'第<span class="subst">&#123;i&#125;</span>页妹纸完成抓取'</span>)</span><br></pre></td></tr></table></figure><h3 id="爬取全国所有城市的名称，包含热门城市和全部城市"><a href="#爬取全国所有城市的名称，包含热门城市和全部城市" class="headerlink" title="爬取全国所有城市的名称，包含热门城市和全部城市"></a>爬取全国所有城市的名称，包含热门城市和全部城市</h3><p>待补充！！！</p><h3 id="梨视频短视频的爬取"><a href="#梨视频短视频的爬取" class="headerlink" title="梨视频短视频的爬取"></a>梨视频短视频的爬取</h3><ul><li><p>需求：爬取 <code>https://www.pearvideo.com/category_8</code> 这一页的一组视频</p></li><li><p>视频的url是藏在script中，所以只能配合正则去解析。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.pearvideo.com/category_8'</span></span><br><span class="line">page_text = requests.get(url,headers=headers).text</span><br><span class="line"><span class="comment"># 数据解析:得到视频的名字与视频详情页的url</span></span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">li_list = tree.xpath(<span class="string">'//*[@id="listvideoListUl"]/li'</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    name = li.xpath(<span class="string">'./div/a/div[2]/text()'</span>)[<span class="number">0</span>] + <span class="string">'.mp4'</span></span><br><span class="line">    detail_url = <span class="string">'https://www.pearvideo.com/'</span> + li.xpath(<span class="string">'./div/a/@href'</span>)[<span class="number">0</span>]</span><br><span class="line">    detail_page_text = requests.get(detail_url,headers=headers).text</span><br><span class="line">    <span class="comment"># 数据解析：得到视频的url</span></span><br><span class="line">    detail_tree = etree.HTML(detail_page_text)</span><br><span class="line">    video = detail_tree.xpath(<span class="string">'//script/text()'</span>)</span><br><span class="line">    st = <span class="string">''</span>.join(video)</span><br><span class="line">    ex = <span class="string">r'srcUrl="(.*?.mp4)"'</span></span><br><span class="line">    <span class="comment"># print(video)</span></span><br><span class="line">    video_url = re.findall(ex,st)[<span class="number">0</span>]</span><br><span class="line">    mp4 = requests.get(video_url,headers=headers).content</span><br><span class="line">    <span class="comment"># 得到视频的bytes流。</span></span><br><span class="line">    <span class="keyword">with</span> open(name,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(mp4)</span><br><span class="line">    print(<span class="string">'下载完成！'</span>)</span><br></pre></td></tr></table></figure><h1 id="requests高级"><a href="#requests高级" class="headerlink" title="requests高级"></a>requests高级</h1><ul><li>下面介绍的本质上也是针对反爬机制做出的策略。</li></ul><h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><ul><li><p>概念：代理服务器</p></li><li><p>代理的作用：</p><ul><li>请求和响应的转发（拦截请求和响应）</li></ul></li><li><p>代理和爬虫之间的关联是什么？</p><ul><li>可以基于代理实现更换爬虫程序请求的ip地址</li></ul></li><li><p>代理ip的网站</p><ul><li>西祠代理 <code>https://www.xicidaili.com/nn</code></li><li>快代理</li><li><a href="http://www.goubanjia.com" target="_blank" rel="noopener">www.goubanjia.com</a></li><li>代理精灵</li></ul></li><li><p>代理的匿名度</p><ul><li>高匿名：不知道是否使用代理服务器，也无法知道最终的ip</li><li>匿名：使用了代理服务器，但不知道最终ip</li><li>透明：</li></ul></li><li><p>类型</p><ul><li>http：只能转发http协议的请求</li><li>https：可以转发https的请求</li></ul></li><li><p>chrome添加代理</p><ul><li>设置搜索 代理</li><li>在连接下，点击局域网设置，点击代理服务器并进行配置。</li></ul></li><li><p>目标：在requests使用代理</p><ul><li>proxies参数: <code>{ &#39;https&#39; : &#39;ip:port&#39; }</code></li></ul></li></ul><h3 id="使用代理ip"><a href="#使用代理ip" class="headerlink" title="使用代理ip"></a>使用代理ip</h3><ul><li>我是在<a href="http://www.jinglingdaili.com/" target="_blank" rel="noopener">代理精灵</a>买的，6块300个，每个可以用5-25分钟。</li><li>先尝试一个，然后爬取 <code>百度搜索ip页面</code> 保存到本地，查看ip是否发生变化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在http://www.jinglingdaili.com/ </span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd=ip'</span></span><br><span class="line">page_text = requests.get(url,headers=headers,proxies=&#123;<span class="string">'https'</span>:<span class="string">'140.250.89.43:31799'</span>&#125;).text</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./ip.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(page_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># note: 这个代理会查看当前用户ip，如果用户ip不在白名单则无法使用，引起ProxyError</span></span><br></pre></td></tr></table></figure><h3 id="搭建付费的代理池"><a href="#搭建付费的代理池" class="headerlink" title="搭建付费的代理池"></a>搭建付费的代理池</h3><ul><li>需求：还是前面的买的那些ip，给代理池中放50个ip</li><li>首先在代理精灵中生成API链接，有了这个链接，我们就可以直接爬取了</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://ip.11jsq.com/index.php/api/entry?method=proxyServer.generate_api_url&amp;packid=2&amp;fa=0&amp;fetch_key=&amp;groupid=0&amp;qty=50&amp;time=1&amp;pro=&amp;city=&amp;port=1&amp;format=txt&amp;ss=1&amp;css=&amp;dt=1&amp;specialTxt=3&amp;specialJson=&amp;usertype=2'</span></span><br><span class="line">page_text = requests.get(url,headers=headers).text</span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">ip_pools = tree.xpath(<span class="string">'//body//text()'</span>)[<span class="number">0</span>].split(<span class="string">'\r\n'</span>)  </span><br><span class="line"><span class="comment"># ip_pools = tree.xpath('//pre/text()')    # 一个疑惑：为什么 //pre/text() 没有结果呢？ 因为你的page_text 和你看到的网页不同</span></span><br><span class="line">print(len(ip_pools))</span><br></pre></td></tr></table></figure><p>note：如果提取不到说句，说明你的xpath表达式不正确，我们参考page_text的输出进行编写。</p><h3 id="搭建一个免费的代理池"><a href="#搭建一个免费的代理池" class="headerlink" title="搭建一个免费的代理池"></a>搭建一个免费的代理池</h3><ul><li>需求：将 <a href="https://www.xicidaili.com/nn" target="_blank" rel="noopener">西祠代理</a> 上面展示的 ip，端口以及类型进行爬取。</li><li>当然，这些数据其实还需要进行测试，待补充！！！</li></ul><p>不使用代理，本机IP直接尝试，但是不到50页就会被封掉IP</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.xicidaili.com/nn/%s'</span>    <span class="comment"># 通用的url模板(不可变)</span></span><br><span class="line">all_ips = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">30</span>):</span><br><span class="line">    new_url = format(url%page)</span><br><span class="line">    page_text = requests.get(new_url,headers=headers).text</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    <span class="comment"># 在xpath表达式中不可以出现tbody标签</span></span><br><span class="line"><span class="comment">#     tr_list = tree.xpath('//*[@id="ip_list"]/tbody/tr')[1:]   # 过滤掉第1个无效的</span></span><br><span class="line">    tr_list = tree.xpath(<span class="string">'//*[@id="ip_list"]//tr'</span>)[<span class="number">1</span>:]   <span class="comment"># 过滤掉第1个无效的</span></span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">        ip = tr.xpath(<span class="string">'./td[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        port = tr.xpath(<span class="string">'./td[3]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        ip_type = tr.xpath(<span class="string">'./td[6]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        all_ips.append(&#123;</span><br><span class="line">            <span class="string">'ip'</span>:ip,</span><br><span class="line">            <span class="string">'port'</span>:port,</span><br><span class="line">            <span class="string">'type'</span>:ip_type,</span><br><span class="line">        &#125;)</span><br><span class="line">print(len(all_ips))    </span><br><span class="line">fin_ips = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来对放在列表中的元素进行测试，如果不行就排除</span></span><br><span class="line">test_url = <span class="string">'https://www.baidu.com/'</span></span><br><span class="line"><span class="keyword">for</span> ip_dic <span class="keyword">in</span> all_ips:  </span><br><span class="line">    r = requests.get(test_url,headers=headers,proxies=&#123;ip_dic[<span class="string">'type'</span>]:ip_dic[<span class="string">'ip'</span>] + <span class="string">':'</span> + ip_dic[<span class="string">'port'</span>]&#125;)</span><br><span class="line">    <span class="keyword">if</span> r.status_code == <span class="number">200</span>:</span><br><span class="line">        fin_ips.append(ip_dic)</span><br><span class="line">print(len(all_ips))</span><br><span class="line">print(len(fin_ips))</span><br></pre></td></tr></table></figure><p>为了爬取大量数据，我们借助上面搭建的代理池再次尝试！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建付费的代理池</span></span><br><span class="line">url = <span class="string">'http://ip.11jsq.com/index.php/api/entry?method=proxyServer.generate_api_url&amp;packid=2&amp;fa=0&amp;fetch_key=&amp;groupid=0&amp;qty=50&amp;time=1&amp;pro=&amp;city=&amp;port=1&amp;format=txt&amp;ss=1&amp;css=&amp;dt=1&amp;specialTxt=3&amp;specialJson=&amp;usertype=2'</span></span><br><span class="line">page_text = requests.get(url,headers=headers).text</span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">ip_pools = tree.xpath(<span class="string">'//body//text()'</span>)[<span class="number">0</span>].split(<span class="string">'\r\n'</span>)  </span><br><span class="line"><span class="comment"># ip_pools = tree.xpath('//pre/text()')    # 一个疑惑：为什么 //pre/text() 没有结果呢？ 因为你的page_text 和你看到的网页不同</span></span><br><span class="line">print(len(ip_pools))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用代理池中的代理，来解决ip限制</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通用url</span></span><br><span class="line">xici_url = <span class="string">'https://www.xicidaili.com/nn/%s'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从西祠代理爬取到的IP池，包含ip，port，type</span></span><br><span class="line">all_ips = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">50</span>):</span><br><span class="line">    new_url = format(xici_url%page)</span><br><span class="line">    page_text = requests.get(new_url,headers=headers,proxies=&#123;<span class="string">'https'</span>:random.choice(ip_pools)&#125;).text</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    <span class="comment"># 在xpath表达式中不可以出现tbody标签</span></span><br><span class="line"><span class="comment">#     tr_list = tree.xpath('//*[@id="ip_list"]/tbody/tr')[1:]   # 过滤掉第1个无效的</span></span><br><span class="line">    tr_list = tree.xpath(<span class="string">'//*[@id="ip_list"]//tr'</span>)[<span class="number">1</span>:]   <span class="comment"># 过滤掉第1个无效的</span></span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">        ip = tr.xpath(<span class="string">'./td[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        port = tr.xpath(<span class="string">'./td[3]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        ip_type = tr.xpath(<span class="string">'./td[6]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        all_ips.append(&#123;</span><br><span class="line">            <span class="string">'ip'</span>:ip,</span><br><span class="line">            <span class="string">'port'</span>:port,</span><br><span class="line">            <span class="string">'type'</span>:ip_type,</span><br><span class="line">        &#125;)</span><br><span class="line">print(len(all_ips)) </span><br><span class="line"><span class="comment"># 接下来对放在列表中的元素进行测试，如果不行就排除</span></span><br><span class="line">test_url = <span class="string">'https://www.baidu.com/'</span></span><br><span class="line"><span class="keyword">for</span> ip_dic <span class="keyword">in</span> all_ips:  </span><br><span class="line">    r = requests.get(test_url,headers=headers,proxies=&#123;ip_dic[<span class="string">'type'</span>]:ip_dic[<span class="string">'ip'</span>] + <span class="string">':'</span> + ip_dic[<span class="string">'port'</span>]&#125;)</span><br><span class="line">    <span class="keyword">if</span> r.status_code == <span class="number">200</span>:</span><br><span class="line">        fin_ips.append(ip_dic)</span><br><span class="line">print(len(all_ips))</span><br><span class="line">print(len(fin_ips))</span><br></pre></td></tr></table></figure><h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>爬虫中怎样处理cookie？</p><ul><li><p>手动处理：</p><ul><li>将cookie写在headers中，（不涉及有效时长以及动态变化）</li></ul></li><li><p>自动处理：</p><ul><li>使用session对象：requests.Session()，它也被称为 <a href="https://requests.kennethreitz.org/zh_CN/latest/user/advanced.html#session-objects" target="_blank" rel="noopener">会话对象</a><ul><li>会话对象让你能够跨请求保持某些参数。它也会在同一个 Session 实例发出的所有请求之间保持 cookie</li></ul></li><li>作用：<ul><li>session对象和requests对象都可以对指定的url进行请求发送。只不过使用session进行请求发送的过程中，如果产生了cookie则cookie会被自动存储到session对象中。</li></ul></li></ul></li></ul><h3 id="爬取雪球网的新闻和内容"><a href="#爬取雪球网的新闻和内容" class="headerlink" title="爬取雪球网的新闻和内容"></a>爬取雪球网的新闻和内容</h3><ul><li>地址：<code>https://xueqiu.com</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误尝试</span></span><br><span class="line"><span class="comment"># 首先我们发现它是一个动态加载的，找到url，尝试拿到这条新闻</span></span><br><span class="line">url = <span class="string">'https://xueqiu.com/v4/statuses/public_timeline_by_category.json?since_id=-1&amp;max_id=20352368&amp;count=15&amp;category=-1'</span></span><br><span class="line">dic = requests.get(url,headers=headers).json()</span><br><span class="line">print(dic)</span><br><span class="line"><span class="comment"># 输出如下：</span></span><br><span class="line"><span class="comment"># &#123;'error_description': '遇到错误，请刷新页面或者重新登录帐号后再试', 'error_uri': '/v4/statuses/public_timeline_by_category.json', 'error_data': None, 'error_code': '400016'&#125;</span></span><br></pre></td></tr></table></figure><p>​ 有了这种提示，说明我们需要使用cookie了，为什么呢？因为你访问的这个地址并不是首页，而是在首页的基础上给后端发送 ajax 请求，所以有了基于cookie的反爬。</p><ul><li>那我们该怎么做呢？是需要实例化 requests.Session() 对象，同requests的使用方式，进行使用。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建会话对象</span></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'https://xueqiu.com'</span>,headers=headers)</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://xueqiu.com/v4/statuses/public_timeline_by_category.json?since_id=-1&amp;max_id=20352368&amp;count=15&amp;category=-1'</span></span><br><span class="line"><span class="comment"># 保证该次请求携带的cookie 才可以请求成功</span></span><br><span class="line">dic = s.get(url,headers=headers).json()</span><br><span class="line">print(dic)</span><br></pre></td></tr></table></figure><p>这回就可以拿到数据了。但是如果由 cookie 检测的时候才会选择使用会话对象，毕竟存在效率问题。</p><h2 id="验证码的识别"><a href="#验证码的识别" class="headerlink" title="验证码的识别"></a>验证码的识别</h2><p>对于验证码我们大多使用在线打码平台，给大家推荐几个：</p><ul><li>超级鹰（正在用）</li><li>云打码</li></ul><p>打码平台（超级鹰）的使用：</p><ul><li>注册</li><li>登录<ul><li>创建一个软件，复制软件ID</li><li>下载示例代码《开发文档》</li></ul></li></ul><p>所以我们需要做的就是将验证码爬取下来，使用打码平台的API的到结果。</p><p>下面这个是超级鹰的内部实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chaojiying_Client</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, soft_id)</span>:</span></span><br><span class="line">        self.username = username</span><br><span class="line">        password =  password.encode(<span class="string">'utf8'</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">'user'</span>: self.username,</span><br><span class="line">            <span class="string">'pass2'</span>: self.password,</span><br><span class="line">            <span class="string">'softid'</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PostPic</span><span class="params">(self, im, codetype)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im: 图片字节</span></span><br><span class="line"><span class="string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'codetype'</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">'userfile'</span>: (<span class="string">'ccc.jpg'</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/Processing.php'</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ReportError</span><span class="params">(self, im_id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im_id:报错题目的图片ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'id'</span>: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/ReportError.php'</span>, data=params, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br></pre></td></tr></table></figure><p>将打码工具封装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">(img_path,ctype)</span>:</span></span><br><span class="line">    chaojiying = Chaojiying_Client(<span class="string">'用户名'</span>, <span class="string">'密码'</span>, <span class="string">'901824'</span>)        <span class="comment">#用户中心&gt;&gt;软件ID 生成一个替换 96001</span></span><br><span class="line">    im = open(img_path, <span class="string">'rb'</span>).read()</span><br><span class="line">    <span class="keyword">return</span> chaojiying.PostPic(im, ctype)[<span class="string">'pic_str'</span>]  <span class="comment">#1902 验证码类型  官方网站&gt;&gt;价格体系 3.4+版 print 后要加()</span></span><br></pre></td></tr></table></figure><h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p>攻克了验证码来尝试一下模拟登录哈！</p><ul><li>地址为：<code>https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到验证码的图片，并存储到本地</span></span><br><span class="line">main_url = <span class="string">'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化会话对象</span></span><br><span class="line">s = requests.Session()</span><br><span class="line">page_text = s.get(main_url,headers=headers).text</span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">img_url = <span class="string">'https://so.gushiwen.org'</span> + tree.xpath(<span class="string">'//*[@id="imgCode"]/@src'</span>)[<span class="number">0</span>] </span><br><span class="line"><span class="comment"># 当然如果我们直接拿验证码的图片会发现，这个地址是直接可以随机生成验证码的 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用超级鹰解决验证码</span></span><br><span class="line">img_data = s.get(img_url).content     <span class="comment"># 这里是坑，原本以为访问首页就能拿到cookie，这里不用会话对象，可实际上cookie是在这里产生的。</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./code.jpg'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(img_data)</span><br><span class="line">code = get_code(<span class="string">'./code.jpg'</span>,<span class="number">1004</span>)</span><br><span class="line">print(code)</span><br><span class="line">__VIEWSTATE = tree.xpath(<span class="string">'//*[@id="__VIEWSTATE"]/@value'</span>)[<span class="number">0</span>]</span><br><span class="line">__VIEWSTATEGENERATOR = tree.xpath(<span class="string">'//*[@id="__VIEWSTATEGENERATOR"]/@value'</span>)[<span class="number">0</span>]</span><br><span class="line">print(__VIEWSTATE)</span><br><span class="line">print(__VIEWSTATEGENERATOR)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只需要给这个地址发post请求</span></span><br><span class="line">login_url = <span class="string">'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'__VIEWSTATE'</span>: __VIEWSTATE,    <span class="comment"># 动态数据，隐藏在前面标签内</span></span><br><span class="line">    <span class="string">'__VIEWSTATEGENERATOR'</span>: __VIEWSTATEGENERATOR,</span><br><span class="line">    <span class="string">'from'</span>: <span class="string">'http://so.gushiwen.org/user/collect.aspx'</span>,</span><br><span class="line">    <span class="string">'email'</span>: <span class="string">'用户名'</span>,</span><br><span class="line">    <span class="string">'pwd'</span>: <span class="string">'密码'</span>,</span><br><span class="line">    <span class="string">'code'</span>: code,</span><br><span class="line">    <span class="string">'denglu'</span>: <span class="string">'登录'</span>,</span><br><span class="line">&#125;</span><br><span class="line">ret = s.post(login_url,data=data,headers=headers).text</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./gushi.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="comment"># 存储到本地并以此判断我们是否成功登录</span></span><br><span class="line">    fp.write(ret)</span><br></pre></td></tr></table></figure><p>这里设计的坑：</p><ol><li>验证码</li><li>动态参数 __VIEWSTATE 等是隐藏在前端页面源码中的</li><li>需要携带cookie才能成功，而设置cookie的地方却在请求验证码。</li></ol><p>所以如果涉及cookie，觉得不确定就都用会话对象处理。</p><h1 id="遇到的错误以及解决方式"><a href="#遇到的错误以及解决方式" class="headerlink" title="遇到的错误以及解决方式"></a>遇到的错误以及解决方式</h1><h2 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h2><ul><li>headers中加 Connection:’close’</li></ul><h2 id="代理错误"><a href="#代理错误" class="headerlink" title="代理错误"></a>代理错误</h2><p><strong>ProxyError</strong>: HTTPSConnectionPool(host=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, port=443): Max retries exceeded with url: /s?tn=80035161_2_dg&amp;wd=ip (Caused by ProxyError(‘Cannot connect to proxy.’, OSError(‘Tunnel connection failed: 503 Service Unavailable’)))</p><ul><li>我的代理并没有生效，这个代理是绑定本机ip的，但是我的ip不再白名单中。</li></ul></div><div><div><div style="text-align:center;color:#555;font-size:14px">-------------The End-------------</div></div></div><div><div class="my_post_copyright"><script src="/js/src/clipboard.min.js"></script><script src="/js/src/jquery2.0/jquery.min.js"></script><script src="/js/src/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/网络爬虫/20190921-Web_Spider_3.html">基于requests的爬虫</a></p><p><span>文章作者:</span><a href="/" title="访问 Naqin 的个人博客">Naqin</a></p><p><span>发布时间:</span>2019年09月21日 - 10:09</p><p><span>最后更新:</span>2019年11月05日 - 01:11</p><p><span>原始链接:</span><a href="/网络爬虫/20190921-Web_Spider_3.html" title="基于requests的爬虫">https://chennq.top/网络爬虫/20190921-Web_Spider_3.html</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://chennq.top/网络爬虫/20190921-Web_Spider_3.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div id="wechat_subscriber" style="display:block;padding:10px 0;margin:20px auto;width:100%;text-align:center"><img id="wechat_subscriber_qcode" src="/uploads/wechat.jpg" alt="Naqin wechat" style="width:200px;max-width:100%"><div>欢迎看官加我微信！</div></div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/uploads/wechatpay.jpg" alt="Naqin 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/学习笔记/" rel="tag"><i class="fa fa-tag"></i> 学习笔记</a> <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a> <a href="/tags/网络爬虫/" rel="tag"><i class="fa fa-tag"></i> 网络爬虫</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/网络爬虫/20190920-Web_Spider__2.html" rel="next" title="爬虫概述"><i class="fa fa-chevron-left"></i> 爬虫概述</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Hexo/20190922-hexo_next_404page.html" rel="prev" title="Hexo-自定义含有小游戏的404页面">Hexo-自定义含有小游戏的404页面 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div id="gitalk-container"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="https://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/tu6.jpg" alt="Naqin"><p class="site-author-name" itemprop="name">Naqin</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">133</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">41</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/atlasnq" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:naqin00@hotmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#概述"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#通用爬虫"><span class="nav-number">2.</span> <span class="nav-text">通用爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#爬取搜狗首页的页面源码数据"><span class="nav-number">2.1.</span> <span class="nav-text">爬取搜狗首页的页面源码数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现一个简易的网页采集器"><span class="nav-number">2.2.</span> <span class="nav-text">实现一个简易的网页采集器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动态加载的数据"><span class="nav-number">2.3.</span> <span class="nav-text">动态加载的数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#爬取肯德基的餐厅位置信息"><span class="nav-number">2.4.</span> <span class="nav-text">爬取肯德基的餐厅位置信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#中标公告提取"><span class="nav-number">2.5.</span> <span class="nav-text">中标公告提取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#爬取图片"><span class="nav-number">2.6.</span> <span class="nav-text">爬取图片</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于requests的图片爬取"><span class="nav-number">2.6.1.</span> <span class="nav-text">基于requests的图片爬取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于urllib的图片爬取"><span class="nav-number">2.6.2.</span> <span class="nav-text">基于urllib的图片爬取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#聚焦爬虫"><span class="nav-number">3.</span> <span class="nav-text">聚焦爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据解析"><span class="nav-number">3.1.</span> <span class="nav-text">数据解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则解析"><span class="nav-number">3.2.</span> <span class="nav-text">正则解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取煎蛋网中的图片"><span class="nav-number">3.2.1.</span> <span class="nav-text">爬取煎蛋网中的图片</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bs4解析"><span class="nav-number">3.3.</span> <span class="nav-text">bs4解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#熟悉"><span class="nav-number">3.3.1.</span> <span class="nav-text">熟悉</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取三国演义小说的标题和内容"><span class="nav-number">3.3.2.</span> <span class="nav-text">爬取三国演义小说的标题和内容</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xpath-解析"><span class="nav-number">3.4.</span> <span class="nav-text">xpath 解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#熟悉-1"><span class="nav-number">3.4.1.</span> <span class="nav-text">熟悉</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取虎牙主播名称，热度和标题"><span class="nav-number">3.4.2.</span> <span class="nav-text">爬取虎牙主播名称，热度和标题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取前5页的妹子"><span class="nav-number">3.4.3.</span> <span class="nav-text">爬取前5页的妹子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取全国所有城市的名称，包含热门城市和全部城市"><span class="nav-number">3.4.4.</span> <span class="nav-text">爬取全国所有城市的名称，包含热门城市和全部城市</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梨视频短视频的爬取"><span class="nav-number">3.4.5.</span> <span class="nav-text">梨视频短视频的爬取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#requests高级"><span class="nav-number">4.</span> <span class="nav-text">requests高级</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#代理"><span class="nav-number">4.1.</span> <span class="nav-text">代理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用代理ip"><span class="nav-number">4.1.1.</span> <span class="nav-text">使用代理ip</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#搭建付费的代理池"><span class="nav-number">4.1.2.</span> <span class="nav-text">搭建付费的代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#搭建一个免费的代理池"><span class="nav-number">4.1.3.</span> <span class="nav-text">搭建一个免费的代理池</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cookie"><span class="nav-number">4.2.</span> <span class="nav-text">cookie</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取雪球网的新闻和内容"><span class="nav-number">4.2.1.</span> <span class="nav-text">爬取雪球网的新闻和内容</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#验证码的识别"><span class="nav-number">4.3.</span> <span class="nav-text">验证码的识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模拟登录"><span class="nav-number">4.4.</span> <span class="nav-text">模拟登录</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#遇到的错误以及解决方式"><span class="nav-number">5.</span> <span class="nav-text">遇到的错误以及解决方式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#超时"><span class="nav-number">5.1.</span> <span class="nav-text">超时</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代理错误"><span class="nav-number">5.2.</span> <span class="nav-text">代理错误</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heartbeat"></i> </span><span class="author" itemprop="copyrightHolder">Naqin</span></div><div class="BbeiAn-info">蒙ICP备 - <a class="beian" target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#f7bb30;text-decoration:none!important;border-bottom:none" rel="nofollow">19005279号 </a>| <a class="beian" target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=15012202000146" style="color:#f7bb30;text-decoration:none!important;border-bottom:none;padding-left:30px;background:url(/images/beian.png) no-repeat left center" rel="nofollow">蒙公网安备 15012202000146号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><link rel="stylesheet" href="/css/gitalk.css"><script src="/js/src/gitalk.min.js"></script><div id="gitalk-container"></div><script src="/js/src/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"377dab7c4f5a8991f28c",clientSecret:"7440711408c01caf520bc000a5a353aef73a142b",repo:"guestbook",owner:"atlasnq",admin:["atlasnq"],id:md5(location.pathname),distractionFreeMode:"true"});gitalk.render("gitalk-container")</script><script type="text/javascript">// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></body></html>